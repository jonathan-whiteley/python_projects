{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"FilterFlightInfo\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"planes1\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_events.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = raw_events.select(raw_events.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|baro_altitude|callsign|geo_altitude|icao24|last_contact|latitude|longitude|on_ground|origin_country|position_source|  spi|      time|time_position|true_track|velocity|vertical_rate|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|       701.04|72150   |      655.32|ae1fa5|  1626911540| 36.2166|-115.2025|    false| United States|              0|false|1626911540|   1626911540|    108.43|    6.51|        -0.98|\n",
      "|      1310.64|N1540M  |     1379.22|a0db6f|  1626911539| 41.5771| -93.5474|    false| United States|              0|false|1626911540|   1626911538|      null|    66.5|        -0.33|\n",
      "|      9631.68|LPE2006 |      4838.7|e8027e|  1626911496|-12.8591| -75.2564|    false|         Chile|              0|false|1626911540|   1626911481|    106.03|  244.07|         8.13|\n",
      "|      5280.66|UAL1626 |     5311.14|aa56db|  1626911539| 29.4873| -95.5204|    false| United States|              0|false|1626911540|   1626911539|    238.25|  211.15|        14.96|\n",
      "|         null|UAL124  |    10988.04|aa9321|  1626911540|  47.092| -63.0908|    false| United States|              0|false|1626911540|   1626911540|     49.01|  280.79|         null|\n",
      "|         null|        |        null|ade18c|  1626911523|  39.873| -75.2434|     true| United States|              0|false|1626911540|   1626911523|     75.94|    0.26|         null|\n",
      "|      11277.6|AAL1378 |     11925.3|ab6fdd|  1626911540| 40.0553| -93.1992|    false| United States|              0|false|1626911540|   1626911540|     69.56|  237.17|         null|\n",
      "|      3337.56|DAL1227 |      3467.1|a90ea8|  1626911539| 39.2914|-119.7123|    false| United States|              0|false|1626911540|   1626911539|    106.97|  158.67|         9.75|\n",
      "|       289.56|N746G   |      281.94|aa096a|  1626911539| 33.2073| -96.5903|    false| United States|              0|false|1626911540|   1626911522|    177.68|    38.1|        -1.95|\n",
      "|      1684.02|KAP619  |     1783.08|ab63b2|  1626911331| 40.1563| -90.8687|    false| United States|              0|false|1626911540|   1626911331|    227.82|   88.87|        -2.93|\n",
      "|      1242.06|N516NG  |     1211.58|a678ec|  1626911539| 33.8628|-117.9072|    false| United States|              0|false|1626911540|   1626911539|    260.84|   48.46|        -0.33|\n",
      "|         null|SWR154F |        null|4b1800|  1626911522| 47.4419|   8.5643|     true|   Switzerland|              0|false|1626911540|   1626911516|    146.25|    0.06|         null|\n",
      "|       9372.6|JST234  |     9113.52|7c6b1b|  1626911540|-42.2961| 173.1346|    false|     Australia|              0|false|1626911540|   1626911540|     21.07|  230.44|         7.48|\n",
      "|       792.48|N454MD  |       838.2|a5820f|  1626911539|  29.852| -98.4407|    false| United States|              0|false|1626911540|   1626911539|    188.79|   57.26|        -0.33|\n",
      "|         null|JST228  |        null|7c6b31|  1626911531|-43.4877| 172.5369|     true|     Australia|              0|false|1626911540|   1626911531|     39.38|    null|         null|\n",
      "|      1889.76|GPD307  |        null|a33b52|  1626911540|  41.292| -70.3741|    false| United States|              0|false|1626911540|   1626911540|     75.75|  135.88|         -7.8|\n",
      "|      10972.8|AAL2172 |    11673.84|a3b88e|  1626911540| 27.7908| -86.2419|    false| United States|              0|false|1626911540|   1626911540|    297.28|  239.06|         null|\n",
      "|      10972.8|AAL1587 |    11643.36|aae316|  1626911539| 35.2733|-101.4872|    false| United States|              0|false|1626911540|   1626911493|    257.51|  263.99|         null|\n",
      "|      4137.66|DAL2247 |     4366.26|a36898|  1626911539| 33.5213| -85.1608|    false| United States|              0|false|1626911540|   1626911539|      null|  199.49|         8.78|\n",
      "|       1066.8|N6036Y  |     1059.18|a7d5a5|  1626911536| 38.6286|-121.1753|    false| United States|              0|false|1626911540|   1626911536|    265.47|   52.12|        -0.65|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode the Json into base dataframe\n",
    "from pyspark.sql.functions import from_json, col\n",
    "json_schema = spark.read.json(events.rdd.map(lambda row: row.value)).schema\n",
    "flight_info = events.rdd.map(lambda x: json.loads(x.value)).toDF(schema=json_schema)\n",
    "flight_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|baro_altitude|callsign|geo_altitude|icao24|last_contact|latitude|longitude|on_ground|origin_country|position_source|  spi|      time|time_position|true_track|velocity|vertical_rate|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|         null|        |        null|ade18c|  1626911523|  39.873| -75.2434|     true| United States|              0|false|1626911540|   1626911523|     75.94|    0.26|         null|\n",
      "|         null|SWR154F |        null|4b1800|  1626911522| 47.4419|   8.5643|     true|   Switzerland|              0|false|1626911540|   1626911516|    146.25|    0.06|         null|\n",
      "|         null|JST228  |        null|7c6b31|  1626911531|-43.4877| 172.5369|     true|     Australia|              0|false|1626911540|   1626911531|     39.38|    null|         null|\n",
      "|         null|AAL736  |        null|aaa6dd|  1626911538| 39.8699| -75.2353|     true| United States|              0|false|1626911540|   1626911538|     75.94|    null|         null|\n",
      "|         null|N603AC  |        null|a7ceb0|  1626911340| 33.6862|-112.0931|     true| United States|              0|false|1626911540|   1626911340|    123.75|    4.89|         null|\n",
      "|         null|AAL947  |        null|ab0361|  1626911536| 38.8548| -77.0412|     true| United States|              0|false|1626911540|   1626911536|    216.56|    null|         null|\n",
      "|         null|RESGAT03|        null|e4940f|  1626911330|-19.8489| -43.9493|     true|        Brazil|              0|false|1626911540|   1626911330|     30.94|    2.06|         null|\n",
      "|         null|N11CP   |        null|a02a1a|  1626911536| 47.5404|-122.3086|     true| United States|              0|false|1626911540|   1626911536|    303.75|    null|         null|\n",
      "|         null|AAL788  |        null|a05776|  1626911278| 33.9371|-118.4164|     true| United States|              0|false|1626911540|   1626911278|     81.56|    10.8|         null|\n",
      "|         null|RM26    |        null|3c407f|  1626911459| 50.0364|   8.5294|     true|       Germany|              0|false|1626911540|   1626911459|    312.19|    8.23|         null|\n",
      "|         null|RM28    |        null|3c4081|  1626911469| 50.0288|  98.5397|     true|       Germany|              0|false|1626911540|   1626911469|     75.94|    1.29|         null|\n",
      "|         null|SWA3442 |        null|a4b259|  1626911540| 39.8618|-104.6726|     true| United States|              0|false|1626911540|   1626911540|    295.31|    1.03|         null|\n",
      "|         null|ANZ1002 |        null|c82439|  1626911479| 33.9381|-118.4266|     true|   New Zealand|              0|false|1626911540|   1626911479|    340.31|    6.69|         null|\n",
      "|         null|SKW3991 |        null|ab161b|  1626911540| 43.1359| -89.3432|     true| United States|              0|false|1626911540|   1626911540|     19.69|    4.89|         null|\n",
      "|         null|SKW3735 |        null|a247f0|  1626911504| 33.9497|-118.4171|     true| United States|              0|false|1626911540|   1626911504|    126.56|    6.69|         null|\n",
      "|         null|ARE4011 |        null|e80209|  1626911536|  4.6997| -74.1549|     true|         Chile|              0|false|1626911540|   1626911536|    126.56|   26.24|         null|\n",
      "|         null|IGO026  |        null|800dac|  1626911472| 25.2576|  55.3468|     true|         India|              0|false|1626911540|   1626911472|    210.94|    null|         null|\n",
      "|         null|DLX805  |        null|aaf4a2|  1626911521| 37.5133|-122.2514|     true| United States|              0|false|1626911540|   1626911521|    253.12|    4.12|         null|\n",
      "|       137.16|RPA5741 |        null|a1b449|  1626911539| 43.1259| -77.6638|     true| United States|              0|false|1626911540|   1626911531|     28.12|    null|         null|\n",
      "|         null|FFT159  |        null|a3b7cb|  1626911539| 43.1378| -89.3452|     true| United States|              0|false|1626911540|   1626911537|    255.94|    null|         null|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_info_on_ground=flight_info.filter(flight_info.on_ground ==True)\n",
    "flight_info_on_ground.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://cloudera/tmp/flight_info_on_ground already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o141.parquet.\n: org.apache.spark.sql.AnalysisException: path hdfs://cloudera/tmp/flight_info_on_ground already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:106)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f8ec6e1a9658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflight_info_on_ground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/flight_info_on_ground\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://cloudera/tmp/flight_info_on_ground already exists.;'"
     ]
    }
   ],
   "source": [
    "flight_info_on_ground.write.parquet(\"/tmp/flight_info_on_ground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://cloudera/tmp/flight_info_off_ground already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o188.parquet.\n: org.apache.spark.sql.AnalysisException: path hdfs://cloudera/tmp/flight_info_off_ground already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:106)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c5a3f7d6885f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mflight_info_off_ground\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflight_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflight_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_ground\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflight_info_off_ground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/flight_info_off_ground\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://cloudera/tmp/flight_info_off_ground already exists.;'"
     ]
    }
   ],
   "source": [
    "flight_info_off_ground=flight_info.filter(flight_info.on_ground !=True)\n",
    "flight_info_off_ground.write.parquet(\"/tmp/flight_info_off_ground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|baro_altitude|callsign|geo_altitude|icao24|last_contact|latitude|longitude|on_ground|origin_country|position_source|  spi|      time|time_position|true_track|velocity|vertical_rate|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|       701.04|72150   |      655.32|ae1fa5|  1626911540| 36.2166|-115.2025|    false| United States|              0|false|1626911540|   1626911540|    108.43|    6.51|        -0.98|\n",
      "|      1310.64|N1540M  |     1379.22|a0db6f|  1626911539| 41.5771| -93.5474|    false| United States|              0|false|1626911540|   1626911538|      null|    66.5|        -0.33|\n",
      "|      5280.66|UAL1626 |     5311.14|aa56db|  1626911539| 29.4873| -95.5204|    false| United States|              0|false|1626911540|   1626911539|    238.25|  211.15|        14.96|\n",
      "|         null|UAL124  |    10988.04|aa9321|  1626911540|  47.092| -63.0908|    false| United States|              0|false|1626911540|   1626911540|     49.01|  280.79|         null|\n",
      "|         null|        |        null|ade18c|  1626911523|  39.873| -75.2434|     true| United States|              0|false|1626911540|   1626911523|     75.94|    0.26|         null|\n",
      "|      11277.6|AAL1378 |     11925.3|ab6fdd|  1626911540| 40.0553| -93.1992|    false| United States|              0|false|1626911540|   1626911540|     69.56|  237.17|         null|\n",
      "|      3337.56|DAL1227 |      3467.1|a90ea8|  1626911539| 39.2914|-119.7123|    false| United States|              0|false|1626911540|   1626911539|    106.97|  158.67|         9.75|\n",
      "|       289.56|N746G   |      281.94|aa096a|  1626911539| 33.2073| -96.5903|    false| United States|              0|false|1626911540|   1626911522|    177.68|    38.1|        -1.95|\n",
      "|      1684.02|KAP619  |     1783.08|ab63b2|  1626911331| 40.1563| -90.8687|    false| United States|              0|false|1626911540|   1626911331|    227.82|   88.87|        -2.93|\n",
      "|      1242.06|N516NG  |     1211.58|a678ec|  1626911539| 33.8628|-117.9072|    false| United States|              0|false|1626911540|   1626911539|    260.84|   48.46|        -0.33|\n",
      "|       792.48|N454MD  |       838.2|a5820f|  1626911539|  29.852| -98.4407|    false| United States|              0|false|1626911540|   1626911539|    188.79|   57.26|        -0.33|\n",
      "|      1889.76|GPD307  |        null|a33b52|  1626911540|  41.292| -70.3741|    false| United States|              0|false|1626911540|   1626911540|     75.75|  135.88|         -7.8|\n",
      "|      10972.8|AAL2172 |    11673.84|a3b88e|  1626911540| 27.7908| -86.2419|    false| United States|              0|false|1626911540|   1626911540|    297.28|  239.06|         null|\n",
      "|      10972.8|AAL1587 |    11643.36|aae316|  1626911539| 35.2733|-101.4872|    false| United States|              0|false|1626911540|   1626911493|    257.51|  263.99|         null|\n",
      "|      4137.66|DAL2247 |     4366.26|a36898|  1626911539| 33.5213| -85.1608|    false| United States|              0|false|1626911540|   1626911539|      null|  199.49|         8.78|\n",
      "|       1066.8|N6036Y  |     1059.18|a7d5a5|  1626911536| 38.6286|-121.1753|    false| United States|              0|false|1626911540|   1626911536|    265.47|   52.12|        -0.65|\n",
      "|       274.32|        |      358.14|a7d5a9|  1626911368| 43.6139| -88.8748|    false| United States|              0|false|1626911540|   1626911368|    177.73|   64.87|        -3.25|\n",
      "|       525.78|N23678  |      563.88|a22238|  1626911350| 42.6348| -83.9695|    false| United States|              0|false|1626911540|   1626911350|    317.05|   40.77|         -2.6|\n",
      "|      1478.28|SKW5406 |     1546.86|ad19f2|  1626911540| 42.0916| -88.1584|    false| United States|              0|false|1626911540|   1626911540|    270.21|  137.36|         -6.5|\n",
      "|      7376.16|UAL1689 |     7787.64|a5954e|  1626911540| 44.2651| -94.6071|    false| United States|              0|false|1626911540|   1626911540|    240.24|  214.53|         5.53|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_info_domestic=flight_info.filter(flight_info.origin_country =='United States')\n",
    "flight_info_domestic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_info_domestic.write.parquet(\"/tmp/flight_info_domestic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_info_international=flight_info.filter(flight_info.origin_country !='United States')\n",
    "flight_info_domestic.write.parquet(\"/tmp/flight_info_international\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
