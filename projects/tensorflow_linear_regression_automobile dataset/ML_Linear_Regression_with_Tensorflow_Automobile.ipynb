{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Linear Regression with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "## Understanding the data\n",
        "Goal: Train models using the [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile)  from 1985 Ward's Automotive Yearbook that is part of the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_auto_data_set_text"
      },
      "source": [
        "### Load the data\n",
        "Load the data using the column names from [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile). We'll only use a few of the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_auto_data_set_code"
      },
      "outputs": [],
      "source": [
        "# Provide the names for the feature columns since the CSV file with the data\n",
        "# does not have a header row.\n",
        "cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n",
        "        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n",
        "        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n",
        "        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n",
        "        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
        "\n",
        "# Load the data from a CSV file into a pandas dataframe. Remember that each row\n",
        "# is an example and each column in a feature.\n",
        "car_data = pd.read_csv(\n",
        "    'https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n",
        "    sep=',', names=cols, header=None, encoding='latin-1')\n",
        "\n",
        "# Display applies built-in formatting for nicer printing, if available.\n",
        "display(car_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvcJJ_rUifF2"
      },
      "source": [
        "### Randomize\n",
        "With SGD (Stochastic Gradient Descent) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative. The original data (above) appears sorted by *make* in alphabetic order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3webN4USifuB"
      },
      "outputs": [],
      "source": [
        "# We want to shuffle the order of the rows without touching the columns.\n",
        "# First, we get a list of indices corresponding to the rows.\n",
        "indices = np.arange(car_data.shape[0])\n",
        "print('indices:', indices, '\\n')\n",
        "\n",
        "# Next, we shuffle the indices using np.random.permutation but set a random seed\n",
        "# so that everyone gets the same results each time.\n",
        "np.random.seed(0)\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "print('shuffled indices:', shuffled_indices, '\\n')\n",
        "\n",
        "# Finally, we use dataframe.reindex to change the ordering of the original\n",
        "# dataframe.\n",
        "car_data = car_data.reindex(shuffled_indices)\n",
        "display(car_data)\n",
        "\n",
        "# Note that this could be done in one fancy line:\n",
        "# car_data = car_data.reindex(np.random.permutation(car_data.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gI95UG0FAW"
      },
      "source": [
        "### Feature selection\n",
        "To keep things simple, I focused on just a few of the 26 columns. Since the values come as strings, we need to convert them to floats. Also, we remove examples (rows) that have some missing value(s) of the columns we care about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwu8udZY0Fkj"
      },
      "outputs": [],
      "source": [
        "# Choose a subset of columns (these are all numeric).\n",
        "columns = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
        "car_data = car_data[columns]\n",
        "\n",
        "# Convert strings to numeric values, coercing missing values to nan.\n",
        "for column in columns:\n",
        "  car_data[column] = pd.to_numeric(car_data[column], errors='coerce')\n",
        "\n",
        "# The dropna function drops rows with missing value(s) by default.\n",
        "car_data = car_data.dropna()\n",
        "\n",
        "# This leaves us with 199 examples.\n",
        "display(car_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S55LmJ9DwRJg"
      },
      "source": [
        "### Train/Test split\n",
        "Now that we've shuffled the order, let's split into portions for train and test easily. \n",
        "\n",
        "We're going to train models that **predict price from the other columns**, so we'll create separate variables for input and output data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj3U4nBMm0QX"
      },
      "outputs": [],
      "source": [
        "# We'll use these input features.\n",
        "features = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']\n",
        "\n",
        "# Use a ~80/20 train/test split.\n",
        "car_train = car_data[:160]\n",
        "car_test = car_data[160:]\n",
        "\n",
        "# Create separate variables for features (inputs) and labels (outputs).\n",
        "# We will be using these in the cells below.\n",
        "car_train_features = car_train[features]\n",
        "car_test_features = car_test[features]\n",
        "car_train_labels = car_train['price']\n",
        "car_test_labels = car_test['price']\n",
        "\n",
        "# Confirm the data shapes are as expected.\n",
        "print('train data shape:', car_train_features.shape)\n",
        "print('train labels shape:', car_train_labels.shape)\n",
        "print('test data shape:', car_test_features.shape)\n",
        "print('test labels shape:', car_test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dGZgYo7gp4X"
      },
      "source": [
        "---\n",
        "### Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_8Yf0t9sTvG"
      },
      "source": [
        "Now that we have test data, we can evaluate a baseline. We'll use the average price of cars in the training set as our baseline model -- that is, the baseline always predicts the average price regardless of the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osBXeXWygp4T"
      },
      "outputs": [],
      "source": [
        "# Implement baseline, average price\n",
        "car_train_baseline = car_train_labels.mean()\n",
        "car_train_baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute Root mean square error (RMSE) for each model\n",
        "print(\n",
        "    'RMSE train baseline model:', np.sqrt(np.mean(np.square(car_train_labels - car_train_baseline)))\n",
        ")\n",
        "\n",
        "test_baseline_RMSE = np.sqrt(np.mean(np.square(car_test_labels - car_train_baseline)))\n",
        "\n",
        "print('RMSE test baseline model:', test_baseline_RMSE)"
      ],
      "metadata": {
        "id": "W7NSPc3BzNWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j-BZ24fkWII"
      },
      "source": [
        "\n",
        "The test RMSE is larger than the train RMSE. This makes sense since the baseline is an average of the prices training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBULSBygp4R"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPQ2gQ4D8Jg6"
      },
      "source": [
        "### Feature histograms\n",
        "It's hard to stare at a matrix of 160x5 numbers (the shape of our training data) and know what to make of it. Plotting feature histograms is a good way to start building intuition about the data. This gives us a sense of the distribution of each feature, but not how the features relate to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTz4yHT0xMUS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 3))\n",
        "for i in range(len(columns)):\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.hist(np.array(car_train[columns[i]]))\n",
        "  plt.title(columns[i])\n",
        "plt.show()\n",
        "\n",
        "display(car_train.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4wvvzKoUIAN"
      },
      "source": [
        "---\n",
        "### Feature correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7UFfgHmsTvH"
      },
      "source": [
        "Using pandas [`corr()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to print all the pairwise correlation coefficients for the columns (use the training data only). See also the [Wikipedia page on correlation](https://en.wikipedia.org/wiki/Correlation) for more background.\n",
        "\n",
        "Then answer the following questions:\n",
        "\n",
        "1. It appears that higher-priced cars have higher or lower fuel efficiency?\n",
        "1. Which two features are likely to be most redundant?\n",
        "1. Which feature is likely to be least useful for predicting price?\n",
        "\n",
        "Extra (ungraded): try using [`sns.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to examine each pair of features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "car_train.corr(method='pearson')"
      ],
      "metadata": {
        "id": "tmDtY1DM-h8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJtwrjdO6TbS"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(car_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9EH9D7Faf9n"
      },
      "source": [
        "\n",
        "1. It appears that higher-priced cars have lower fuel efficiency, looking at both city-mpg and highway-mpg\n",
        "2. City-mpg and highway-mpg appear to be most redudant as they are almost perfectly correlated, with a 0.97 corr coeff.\n",
        "3. Peak-rpm appears to be least useful for predicting Price, given its small corr coeff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxOhpvdW6TbX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDsxLnljlp0C"
      },
      "source": [
        "## Tensorflow\n",
        "\n",
        "Let's train a linear regression model using Tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjIwkSWBRp_i"
      },
      "source": [
        "### Build a model\n",
        "First, we build a *computational graph*, and then send data through it.\n",
        "\n",
        "Here, we're using [`keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) to create a model layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfdRzjk-RgpG"
      },
      "outputs": [],
      "source": [
        "def build_model(num_features, learning_rate):\n",
        "  \"\"\"Build a TF linear regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    num_features: The number of input features.\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  # This is not strictly necessary, but each time you build a model, TF adds\n",
        "  # new nodes (rather than overwriting), so the colab session can end up\n",
        "  # storing lots of copies of the graph when you only care about the most\n",
        "  # recent. Also, as there is some randomness built into training with SGD,\n",
        "  # setting a random seed ensures that results are the same on each identical\n",
        "  # training run.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  # Build a model using keras.Sequential. While this is intended for neural\n",
        "  # networks (which may have multiple layers), we want just a single layer for\n",
        "  # linear regression.\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim\n",
        "      input_shape=[num_features],  # input dim\n",
        "      use_bias=True,               # use a bias (intercept) param\n",
        "      kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
        "      bias_initializer=tf.ones_initializer,    # initialize bias to 1\n",
        "  ))\n",
        "\n",
        "  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch\n",
        "  # SGD. We can specify the batch size to use for training later.\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "  # Finally, we compile the model. This finalizes the graph for training.\n",
        "  # We specify the MSE loss.\n",
        "  model.compile(loss='mse', optimizer=optimizer)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oweNCtkrfSSm"
      },
      "source": [
        "After we've built a model, we can inspect the initial parameters (weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deD_nYrTfSvc"
      },
      "outputs": [],
      "source": [
        "# Build a model.\n",
        "model = build_model(num_features=1, learning_rate=0.0001)\n",
        "\n",
        "# Use get_weights() which returns lists of weights and biases for the layer.\n",
        "weights, biases = model.layers[0].get_weights()\n",
        "print('Weights:', weights)\n",
        "print('Biases:', biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob6MerNohWjh"
      },
      "outputs": [],
      "source": [
        "# Build a model and look at the initial parameter values.\n",
        "model = build_model(num_features=2, learning_rate=0.0001)\n",
        "weights, biases = model.layers[0].get_weights()\n",
        "print('Weights:', weights)\n",
        "print('Biases:', biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymSX7q8zh9AJ"
      },
      "source": [
        "### Train a model\n",
        "Now let's actually train a model, initially with just 1 feature -- the horsepower. We use the `fit` function as it can take pandas DataFrame objects for input (x) and output (y). In addition, we can convert the return value into a DataFrame that tracks training metrics (in this case, training data loss and validation data loss) after each *epoch* (a full pass through the training data).\n",
        "\n",
        "With SGD each time the model estimates the loss for the current weights, it randomly samples a batch of training examples to do so.\n",
        "\n",
        "Finally, we'll reserve some more examples (taken out of the training set) as a *validation set*. We use this data to check for overfitting while training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixvnITrVf4r3"
      },
      "outputs": [],
      "source": [
        "model = build_model(num_features=1, learning_rate=0.00001)\n",
        "\n",
        "history = model.fit(\n",
        "  x = car_train_features[['horsepower']],\n",
        "  y = car_train_labels,\n",
        "  validation_split=0.1,  # use 10% of the examples as a validation set\n",
        "  epochs=5,\n",
        "  batch_size=32,\n",
        "  verbose=0)\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the loss after each\n",
        "# epoch. The history includes training data loss ('loss') and validation data\n",
        "# loss ('val_loss').\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_CdHB5ol6Gw"
      },
      "source": [
        "---\n",
        "### Feature normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6KoM-78sTvK"
      },
      "source": [
        "Apply mean and variance normalization to produce car_train_features_norm and car_test_features_norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9KKnJ9ymfqc"
      },
      "outputs": [],
      "source": [
        "car_train_features_norm = (car_train_features - car_train_features.mean())/car_train_features.std()\n",
        "car_test_features_norm = (car_test_features - car_train_features.mean())/car_train_features.std()\n",
        "\n",
        "display(car_train_features_norm.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7PosToNsTvK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW0Kv3WLe1ZG"
      },
      "source": [
        "### Training with features\n",
        "Let's test out different sets of input features. To start, here's a simple function that plots train and validation set loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Bo1FKqe8B2"
      },
      "outputs": [],
      "source": [
        "def plot_loss(model, history):\n",
        "  \"\"\"Plot the loss after each training epoch.\"\"\"\n",
        "  # Convert the history object into a DataFrame.\n",
        "  history = pd.DataFrame(history.history)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(range(len(history)), history['loss'], marker='.', color='black')\n",
        "  plt.plot(range(len(history)), history['val_loss'], marker='.', color='red')\n",
        "  plt.legend(['train loss', 'validation loss'])\n",
        "  plt.show()\n",
        "\n",
        "  # Show the final train loss value and the learned model weights.\n",
        "  print('Final train loss:', list(history['loss'])[-1])\n",
        "  print('Final weights:', model.layers[0].get_weights())\n",
        "\n",
        "  ## Show final val loss\n",
        "  print('Final validation loss:', list(history['val_loss'])[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5rwUw5FGae6"
      },
      "source": [
        "---\n",
        "### Adjusting learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLoKP82ysTvK"
      },
      "source": [
        "Retrain the model with different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_list = [0.001, 0.01, 0.1, 1]\n",
        "\n",
        "# try learning rates\n",
        "for rate in learning_rate_list:\n",
        "  model = build_model(num_features=1, learning_rate=rate)\n",
        "\n",
        "  history = model.fit(\n",
        "    # use the normalized features prepared above\n",
        "    x = car_train_features_norm[['horsepower']],\n",
        "    y = car_train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    verbose=0)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"Learning rate: \", rate)\n",
        "  plot_loss(model, history)"
      ],
      "metadata": {
        "id": "So3tp10Fi1tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNV4KM1QIBti"
      },
      "source": [
        "---\n",
        "\n",
        "A learning rate of 0.01 contributes the lowest validation loss after 150 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDXOSmd-h-rA"
      },
      "source": [
        "### Adding features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeluO_UgsTvL"
      },
      "source": [
        "Let's test 4 models:\n",
        "1. features = horsepower\n",
        "2. features = horsepower, peak-rpm\n",
        "3. features = horsepower, peak-rpm, highway-mpg\n",
        "4. features = horsepower, peak-rpm, highway-mpg, city-mpg\n",
        "\n",
        "For consistency, we will use a batch size of 32, 150 epochs, and the best learning rate of 0.01 from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yarz9pGh-rC"
      },
      "outputs": [],
      "source": [
        "# features = ['horsepower', 'peak-rpm', 'highway-mpg', 'city-mpg']\n",
        "\n",
        "def run_experiment(features, learning_rate):\n",
        "  model = build_model(len(features), learning_rate)\n",
        "\n",
        "  history = model.fit(\n",
        "    x = car_train_features_norm[features],\n",
        "    y = car_train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    verbose=0)\n",
        "\n",
        "  plot_loss(model, history)\n",
        "\n",
        "  # Make predictions on test data\n",
        "  test_loss = model.evaluate(car_test_features_norm[features],\n",
        "                             car_test_labels,\n",
        "                             verbose=0)\n",
        "  test_rmse = np.sqrt(test_loss)\n",
        "  print('Test rmse:', test_rmse)\n",
        "\n",
        "  # return so we can store in list\n",
        "  return test_rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['horsepower', 'peak-rpm', 'highway-mpg', 'city-mpg']\n",
        "\n",
        "test_rmse_all = []\n",
        "\n",
        "# loop through features list adding one more to model each time\n",
        "for i in range(len(features)):\n",
        "  print(\"\\n\")\n",
        "  print(\"Features in model: \", features[:i+1])\n",
        "  res = run_experiment(features[:i+1], 0.01)  \n",
        "\n",
        "  test_rmse_all.append(res)"
      ],
      "metadata": {
        "id": "ozCoUMAftsKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepend Baseline model rmse to list\n",
        "test_rmse_all.insert(0, test_baseline_RMSE)\n",
        "test_rmse_all"
      ],
      "metadata": {
        "id": "sEfXENIFwj4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBcyy0oTh-rF"
      },
      "source": [
        "\n",
        "\n",
        "Model | Test RMSE\n",
        "--- | ---\n",
        "Baseline | \n",
        "Horsepower | \n",
        "  + Peak-RPM | \n",
        "  + Highway-MPG | \n",
        "  + City-MPG | \n",
        "\n",
        "We can see below that the 4th model, which includes up to the Highway-MPG feature, has the lowest RMSE. This makes sense with our earlier intuition that the City-MPG feature is highly correlated with Highway-MPG and would likely not contribute meaningfully to our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer in table form\n",
        "\n",
        "model_names = [\"Baseline\", \"Horsepower\", \"+ Peak-RPM\" , \"+ Highway-MPG\" , \"+ City-MPG\"]\n",
        "model_output_df = pd.DataFrame(list(zip(model_names, test_rmse_all)),\n",
        "                               columns =['Model', 'Test RMSE'])\n",
        "model_output_df"
      ],
      "metadata": {
        "id": "JY_ocz8dykiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1bQOw8FsTvL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNWpEPgZjyNL"
      },
      "source": [
        "## Takeways\n",
        "* The **[Pandas](https://pandas.pydata.org/) library** is very useful for manipulating datasets and works well with numpy.\n",
        "* Use a random split into train and test data and measure performance on the test data, starting from a simple **baseline**.\n",
        "* Examine data using histograms and correlations to help build intuition before training any models.\n",
        "* **Tensorflow** works by first building a **computational graph**; then, you can pass data through the graph to produce predictions, updating parameters via gradient descent in training mode; we use the **Keras API** to easily configure models.\n",
        "* Training is often quite sensitive to the **learning rate** hyperparameter, and feature normalization is an important strategy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g8WipM_ImJnn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ML Linear Regression with Tensorflow Automobile.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}