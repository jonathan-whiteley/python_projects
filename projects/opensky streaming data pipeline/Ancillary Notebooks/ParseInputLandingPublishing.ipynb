{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"planes\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = raw_events.select(raw_events.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract question level information\n",
    "def extract_info_level1(row):\n",
    "    data = json.loads(row.value)\n",
    "    result_list = []\n",
    "    \n",
    "    for flight_info in data[\"states\"]:\n",
    "        flight_info_row={\"time\":data[\"time\"],\n",
    "                        \"icao24\" : flight_info[0],\n",
    "                        \"callsign\": flight_info[1],\n",
    "                        \"origin_country\": flight_info[2],\n",
    "                        \"time_position\": flight_info[3],\n",
    "                        \"last_contact\":flight_info[4],\n",
    "                        \"longitude\":flight_info[5],\n",
    "                         \"latitude\":flight_info[6],\n",
    "                         \"baro_altitude\":flight_info[7],\n",
    "                         \"on_ground\":flight_info[8],\n",
    "                         \"velocity\":flight_info[9],\n",
    "                         \"true_track\":flight_info[10],\n",
    "                         \"vertical_rate\":flight_info[11],\n",
    "#                          \"sensors\":flight_info[12],\n",
    "                         \"geo_altitude\":flight_info[13],\n",
    "#                          \"squawk\":flight_info[14],\n",
    "                         \"spi\":flight_info[15],\n",
    "                         \"position_source\":flight_info[16]\n",
    "                        }\n",
    "        result_list.append(Row(**flight_info_row))\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "attribute of type 'Column' is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-60ff68fd7146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m words = events.select(\n\u001b[1;32m      3\u001b[0m    explode(\n\u001b[0;32m----> 4\u001b[0;31m        \u001b[0;34m**\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m    ).alias(\"word\")\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: attribute of type 'Column' is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "words = events.select(\n",
    "   explode(\n",
    "       **events.value\n",
    "   ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|baro_altitude|callsign|geo_altitude|icao24|last_contact|latitude|longitude|on_ground|origin_country|position_source|  spi|      time|time_position|true_track|velocity|vertical_rate|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "|      10972.8|SKW3768 |    11536.68|a2e5ec|  1627273100| 43.4066|-122.0168|    false| United States|              0|false|1627273100|   1627273099|     350.4|  246.79|         0.33|\n",
      "|      2072.64|JST963  |     2110.74|7c6b2f|  1627273099| -34.675| 138.3818|    false|     Australia|              0|false|1627273100|   1627273099|    192.37|  184.86|        -10.4|\n",
      "|         null|        |        null|ade18c|  1627273001| 32.7337| -117.202|     true| United States|              0|false|1627273100|   1627273001|    143.44|    null|         null|\n",
      "|         null|SKW138H |    11262.36|a305fd|  1627272898| 42.8563|-121.2507|    false| United States|              0|false|1627273100|   1627272898|    168.27|  222.77|         null|\n",
      "|         null|SWR1059 |        null|4b1803|  1627273096| 47.4414|   8.5636|     true|   Switzerland|              0|false|1627273100|   1627273091|     64.69|    null|         null|\n",
      "+-------------+--------+------------+------+------------+--------+---------+---------+--------------+---------------+-----+----------+-------------+----------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assume that keen_id is the prime key of level0, sequance id is the prime key in sequences netted info. \n",
    "# and question_id is the prime key for questions\n",
    "flight_status_info=events.rdd.flatMap(extract_info_level1).toDF()\n",
    "flight_status_info.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the result into hadoop\n",
    "flight_status_info.write.parquet(\"/tmp/flight_status_info_table\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # need to create a topic called planes1\n",
    "# # send the result to kafka and save at same time save in hadoop\n",
    "\n",
    "# query = flight_status_info \\\n",
    "#   .selectExpr(\"CAST(icao24 AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "#   .write\\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "#   .option(\"topic\", \"planes1\") \\\n",
    "#   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_info_on_ground=flight_status_info.filter(flight_status_info.on_ground ==True)\n",
    "flight_info_on_ground.write.parquet(\"/tmp/flight_info_on_ground\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_info_off_ground=flight_status_info.filter(flight_status_info.on_ground !=True)\n",
    "flight_info_off_ground.write.parquet(\"/tmp/flight_info_off_ground\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_info_domestic=flight_status_info.filter(flight_status_info.origin_country =='United States')\n",
    "flight_info_domestic.write.parquet(\"/tmp/flight_info_domestic\",mode='append')\n",
    "flight_info_international=flight_status_info.filter(flight_status_info.origin_country !='United States')\n",
    "flight_info_domestic.write.parquet(\"/tmp/flight_info_international\",mode='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_status_info.registerTempTable(\"flight_status_events\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        create external table flight_status\n",
    "        stored as parquet\n",
    "        location '/tmp/flight_status_info_hive'\n",
    "        as\n",
    "        select * from flight_status_events\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
